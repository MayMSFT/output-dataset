{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License [2017] Zalando SE, https://tech.zalando.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple ML pipeline for image classification\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and Keras on Azure Machine Learning. Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Create the Fashion MNIST dataset\n",
    "> * Create a machine learning pipeline to train a simple deep learning neural network on a remote cluster\n",
    "> * Retrieve input datasets from the experiment and register the output model with datasets\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the latest version of AzureML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install private build with output dataset feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --extra-index-url https://azuremlsdktestpypi.azureedge.net/Create-Dev-Index/15335858/ --pre \"azureml-sdk[automl]<0.1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  0.1.0.15772943\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, ComputeTarget, RunConfiguration, Experiment\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: playground-scus\n",
      "Azure region: southcentralus\n",
      "Subscription id: 321cae6f-e4d3-40bf-824f-c07493a62af5\n",
      "Resource group: aml-testing-scus\n"
     ]
    }
   ],
   "source": [
    "# load workspace\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment and a directory\n",
    "\n",
    "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ML experiment\n",
    "exp = Experiment(workspace=workspace, name='keras-mnist-fashion')\n",
    "\n",
    "# create a directory\n",
    "script_folder = './keras-mnist-fashion'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-06-16T15:22:50.161000+00:00', 'errors': None, 'creationTime': '2020-06-16T14:53:20.511976+00:00', 'modifiedTime': '2020-06-16T14:53:37.718473+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT7200S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_DS3_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster-1\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Fashion MNIST dataset\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. \n",
    "\n",
    "Every workspace comes with a default [datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data) (and you can register more) which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and create a dataset from it. We will now upload the [Fashion MNIST](./keras-mnist-fashion) to the default datastore (blob) within your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adls gen 2 datastore already registered with the workspace\n",
    "datastore = workspace.datastores['may_adlsgen2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create an unregistered FileDataset pointing to the path in the datastore. You can also create a dataset from multiple paths. [Learn More](https://aka.ms/azureml/howto/createdatasets) \n",
    "\n",
    "**Note:** Before running the cell below, make sure you uploaded the the data (t10k-images-idx3-ubyte, t10k-labels-idx1-ubyte, train-images-idx3-ubyte, train-labels-idx1-ubyte) to the `keras-mnist-fashion` folder in your adlsgen2 datastore. You can do so via storage explorer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/t10k-images-idx3-ubyte',\n",
       " '/t10k-labels-idx1-ubyte',\n",
       " '/train-images-idx3-ubyte',\n",
       " '/train-labels-idx1-ubyte']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_ds = Dataset.File.from_files([(datastore, 'keras-mnist-fashion')])\n",
    "\n",
    "# list the files referenced by fashion_ds\n",
    "fashion_ds.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-step ML pipeline\n",
    "\n",
    "The [Azure Machine Learning Pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines) enables data scientists to create and manage multiple simple and complex workflows concurrently. A typical pipeline would have multiple tasks to prepare data, train, deploy and evaluate models. Individual steps in the pipeline can make use of diverse compute options (for example: CPU for data preparation and GPU for training) and languages. [Learn More](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines)\n",
    "\n",
    "\n",
    "### Step 1: data preparation\n",
    "\n",
    "In step one, we will load the image and labels from Fashion MNIST dataset into mnist_train.csv and mnist_test.csv\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. Both mnist_train.csv and mnist_test.csv contain 785 columns. The first column consists of the class labels, which represent the article of clothing. The rest of the columns contain the pixel-values of the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the compute environment to install required packages\n",
    "conda = CondaDependencies.create(pip_packages=['azureml-sdk<0.1.1'],\n",
    "                                 pip_indexurl='https://azuremlsdktestpypi.azureedge.net/Create-Dev-Index/15335858/')\n",
    "\n",
    "conda.set_pip_option('--pre')\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.python.conda_dependencies = conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data (or output of a step) is represented by a `OutputFileDatasetConfig` object. preprared_fashion_ds is produced as the output of step 1, and used as the input of step 2. `OutputFileDatasetConfig` introduces a data dependency between steps, and creates an implicit execution order in the pipeline. You can register a `OutputFileDatasetConfig` as a dataset and version the output data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OutputFileDatasetConfig in module azureml.data.output_dataset_config:\n",
      "\n",
      "class OutputFileDatasetConfig(OutputDatasetConfig)\n",
      " |  .. note::This is an experimental class, and may change at any time. For more information, see (https://aka.ms/azuremlexperimental).Represent how to copy the output of a run and be promoted as a FileDataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OutputFileDatasetConfig\n",
      " |      OutputDatasetConfig\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name=None, destination=None, source=None)\n",
      " |      Initialize a OutputFileDatasetConfig.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |          You can pass the OutputFileDatasetConfig as an argument to your run, and it will be automatically\n",
      " |          translated into local path on the compute. The source argument will be used if one is specified,\n",
      " |          otherwise we will automatically generate a directory in the OS's temp folder. The files and folders inside\n",
      " |          the source directory will then be copied to the destination based on the output configuration.\n",
      " |      \n",
      " |          By default the mode by which the output will be copied to the destination storage will be set to mount.\n",
      " |          For more information about mount mode, please see the documentation for as_mount.\n",
      " |      \n",
      " |      :param name: The name of the output specific to this run. This is generally used for lineage purposes. If set\n",
      " |          to None, we will automatically generate a name.\n",
      " |      :type name: str\n",
      " |      :param destination: The destination to copy the output to. If set to None, we will copy the output to the\n",
      " |          workspaceblobstore datastore, under the path /dataset/{run-id}/{output-name}, where `run-id` is the Run's\n",
      " |          ID and the `output-name` is the output name from the `name` parameter above. The destination is a tuple\n",
      " |          where the first item is the datastore and the second item is the path within the datastore to copy the\n",
      " |          data to.\n",
      " |      \n",
      " |          The path within the datastore can be a template path. A template path is just a regular path but with\n",
      " |          placeholders inside. Those placeholders will then be resolved at the appropriate time. The syntax for\n",
      " |          placeholders is {placeholder}, for example, /path/with/{placeholder}. Currently only two placeholders\n",
      " |          are supported, {run-id} and {output-name}.\n",
      " |      :type destination: tuple\n",
      " |      :param source: The path with in the compute target to copy the data to the destination. If set to None, we\n",
      " |          will set this to a directory we create inside the compute target's OS temporary directory.\n",
      " |      :type source: str\n",
      " |  \n",
      " |  as_input(self, name=None)\n",
      " |      Specify how to consume the output as an input in subsequent pipeline steps.\n",
      " |      \n",
      " |      :param name: The name of the input specific to the run.\n",
      " |      :type name: str\n",
      " |      :return: A :class:`azureml.data.dataset_consumption_config.DatasetConsumptionConfig` instance describing\n",
      " |          how to deliver the input data.\n",
      " |      :rtype: azureml.data.dataset_consumption_config.DatasetConsumptionConfig\n",
      " |  \n",
      " |  as_mount(self)\n",
      " |      Set the mode of the output to mount.\n",
      " |      \n",
      " |      For mount mode, the output directory will be a FUSE mounted directory. Files written to the mounted directory\n",
      " |      will be uploaded when the file is closed.\n",
      " |      \n",
      " |      :return: A :class:`azureml.data.OutputFileDatasetConfig` instance with mode set to mount.\n",
      " |      :rtype: azureml.data.OutputFileDatasetConfig\n",
      " |  \n",
      " |  as_upload(self, overwrite=False, source_globs=None)\n",
      " |      Set the mode of the output to upload.\n",
      " |      \n",
      " |      For upload mode, files written to the output directory will be uploaded at the end of the job. If the job\n",
      " |      fails or gets canceled, then the output directory will not be uploaded.\n",
      " |      \n",
      " |      :param overwrite: Whether to overwrite files that already exists in the destination.\n",
      " |      :type overwrite: bool\n",
      " |      :param source_globs: Glob patterns used to filter files that will be uploaded.\n",
      " |      :type source_globs: builtin.list[str]\n",
      " |      :return: A :class:`azureml.data.OutputFileDatasetConfig` instance with mode set to upload.\n",
      " |      :rtype: azureml.data.OutputFileDatasetConfig\n",
      " |  \n",
      " |  read_delimited_files(self, include_path=False, separator=',', header=<PromoteHeadersBehavior.ALL_FILES_HAVE_SAME_HEADERS: 3>, partition_format=None, path_glob=None, set_column_types=None)\n",
      " |      Transform the output dataset to a tabular dataset by reading all the output as delimited files.\n",
      " |      \n",
      " |      :param include_path: Boolean to keep path information as column in the dataset. Defaults to False.\n",
      " |          This is useful when reading multiple files, and want to know which file a particular record\n",
      " |          originated from, or to keep useful information in file path.\n",
      " |      :type include_path: bool\n",
      " |      :param separator: The separator used to split columns.\n",
      " |      :type separator: str\n",
      " |      :param header: Controls how column headers are promoted when reading from files. Defaults to assume\n",
      " |          that all files have the same header.\n",
      " |      :type header: azureml.data.dataset_type_definitions.PromoteHeadersBehavior\n",
      " |      :param partition_format: Specify the partition format of path. Defaults to None.\n",
      " |          The partition information of each path will be extracted into columns based on the specified format.\n",
      " |          Format part '{column_name}' creates string column, and '{column_name:yyyy/MM/dd/HH/mm/ss}' creates\n",
      " |          datetime column, where 'yyyy', 'MM', 'dd', 'HH', 'mm' and 'ss' are used to extract year, month, day,\n",
      " |          hour, minute and second for the datetime type. The format should start from the position of first\n",
      " |          partition key until the end of file path.\n",
      " |          For example, given the path '../USA/2019/01/01/data.csv' where the partition is by country and\n",
      " |          time, partition_format='/{Country}/{PartitionDate:yyyy/MM/dd}/data.csv' creates string column 'Country'\n",
      " |          with value 'USA' and datetime column 'PartitionDate' with value '2019-01-01'.\n",
      " |      :type partition_format: str\n",
      " |      :param path_glob: A glob pattern to filter files that will be read as delimited files. If set to None, then\n",
      " |          all files will be read as delimited files.\n",
      " |      :type path_glob: str\n",
      " |      :param set_column_types: A dictionary to set column data type, where key is column name and value is\n",
      " |          :class:`azureml.data.DataType`. Columns not in the dictionary will remain of type string. Passing None\n",
      " |          will result in no conversions. Entries for columns not found in the source data will not cause an error\n",
      " |          and will be ignored.\n",
      " |      :type set_column_types: dict[str, azureml.data.DataType]\n",
      " |      :return: A :class:`azureml.data.output_dataset_config.OutputTabularDatasetConfig` instance with instruction of\n",
      " |          how to convert the output into a TabularDataset.\n",
      " |      :rtype: azureml.data.output_dataset_config.OutputTabularDatasetConfig\n",
      " |  \n",
      " |  read_parquet_files(self, include_path=False, partition_format=None, path_glob=None, set_column_types=None)\n",
      " |      Transform the output dataset to a tabular dataset by reading all the output as delimited files.\n",
      " |      \n",
      " |      The tabular dataset is created by parsing the parquet file(s) pointed to by the intermediate output.\n",
      " |      \n",
      " |      :param include_path: Boolean to keep path information as column in the dataset. Defaults to False.\n",
      " |          This is useful when reading multiple files, and want to know which file a particular record\n",
      " |          originated from, or to keep useful information in file path.\n",
      " |      :type include_path: bool\n",
      " |      :param partition_format: Specify the partition format of path. Defaults to None.\n",
      " |          The partition information of each path will be extracted into columns based on the specified format.\n",
      " |          Format part '{column_name}' creates string column, and '{column_name:yyyy/MM/dd/HH/mm/ss}' creates\n",
      " |          datetime column, where 'yyyy', 'MM', 'dd', 'HH', 'mm' and 'ss' are used to extract year, month, day,\n",
      " |          hour, minute and second for the datetime type. The format should start from the position of first\n",
      " |          partition key until the end of file path.\n",
      " |          For example, given the path '../USA/2019/01/01/data.parquet' where the partition is by country/region and\n",
      " |          time, partition_format='/{CountryOrRegion}/{PartitionDate:yyyy/MM/dd}/data.csv' creates a string column\n",
      " |          'CountryOrRegion' with the value 'USA' and a datetime column 'PartitionDate' with the value '2019-01-01'.\n",
      " |      :type partition_format: str\n",
      " |      :param path_glob: A glob pattern to filter files that will be read as parquet files. If set to None, then\n",
      " |          all files will be read as parquet files.\n",
      " |      :type path_glob: str\n",
      " |      :param set_column_types: A dictionary to set column data type, where key is column name and value is\n",
      " |          :class:`azureml.data.DataType`. Columns not in the dictionary will remain of type loaded from the parquet\n",
      " |          file. Passing None will result in no conversions. Entries for columns not found in the source data will\n",
      " |          not cause an error and will be ignored.\n",
      " |      :type set_column_types: dict[str, azureml.data.DataType]\n",
      " |      :return: A :class:`azureml.data.output_dataset_config.OutputTabularDatasetConfig` instance with instruction of\n",
      " |          how to convert the output into a TabularDataset.\n",
      " |      :rtype: azureml.data.output_dataset_config.OutputTabularDatasetConfig\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from OutputDatasetConfig:\n",
      " |  \n",
      " |  register_on_complete(self, name, description=None, tags=None)\n",
      " |      Register the output as a new version of a named Dataset after the run has ran.\n",
      " |      \n",
      " |      If the named Dataset does not exists, it will be created and a new version will be created for the newly\n",
      " |      created named Dataset.\n",
      " |      \n",
      " |      :param name: The Dataset name to register the output under.\n",
      " |      :type name: str\n",
      " |      :param description: The description for the Dataset.\n",
      " |      :type description: str\n",
      " |      :param tags: A list of tags to be assigned to the Dataset.\n",
      " |      :type tags: dict[str, str]\n",
      " |      :return: A new :class:`azureml.data.output_dataset_config.OutputDatasetConfig` instance with the registration\n",
      " |          information.\n",
      " |      :rtype: azureml.data.output_dataset_config.OutputDatasetConfig\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OutputDatasetConfig:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "# learn more about the output config\n",
    "help(OutputFileDatasetConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - This is an experimental class, and may change at any time. For more information, see (https://aka.ms/azuremlexperimental).\n",
      "WARNING - This is an experimental class, and may change at any time. For more information, see (https://aka.ms/azuremlexperimental).\n",
      "WARNING - This is an experimental class, and may change at any time. For more information, see (https://aka.ms/azuremlexperimental).\n"
     ]
    }
   ],
   "source": [
    "# write output to adlsgen2 datastore under folder `outputdataset` and registger it as a dataset after the experiment completes\n",
    "# make sure the service principal in your adlsgen2 datastore has blob data contributor role in order to write data back\n",
    "prepared_fashion_ds = OutputFileDatasetConfig(destination=(datastore, 'outputdataset/{run-id}')).register_on_complete(name='prepared_fashion_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify requirements for the PythonScriptStep, such as conda dependencies and docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example is less clear without using argparse and things like arguments = ['--input-dataset', fashion_ds.as_named..., '--output-...']\n",
    "prep_step = PythonScriptStep(name='prepare step',\n",
    "                             script_name=\"prepare.py\",\n",
    "                             # mount fashion_ds dataset to the compute_target\n",
    "                             arguments=[fashion_ds.as_named_input('fashion_ds').as_mount(), prepared_fashion_ds],\n",
    "                             source_directory=script_folder,\n",
    "                             compute_target=compute_target,\n",
    "                             runconfig=run_config,\n",
    "                             allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: train CNN with Keras\n",
    "\n",
    "Next, we construct an `azureml.train.dnn.TensorFlow` estimator object. The TensorFlow estimator is providing a simple way of launching a TensorFlow training job on a compute target. It will automatically provide a docker image that has TensorFlow installed.\n",
    "\n",
    "[EstimatorStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.estimator_step.estimatorstep?view=azure-ml-py) adds a step to run Tensorflow Estimator in a Pipeline. It takes a dataset as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env = Environment('conda-env')\n",
    "conda_env.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk<0.1.1','keras','tensorflow','numpy','scikit-learn', 'matplotlib','pandas'],\n",
    "                                                               pip_indexurl='https://azuremlsdktestpypi.azureedge.net/Create-Dev-Index/15335858/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - This is an experimental class, and may change at any time. For more information, see (https://aka.ms/azuremlexperimental).\n",
      "WARNING - This is an experimental class, and may change at any time. For more information, see (https://aka.ms/azuremlexperimental).\n"
     ]
    }
   ],
   "source": [
    "# No big deal but we're talking above about using the TensorFlow estimator... and we're not using it.\n",
    "from azureml.train.estimator import Estimator\n",
    "# set up training step with Estimator\n",
    "est = Estimator(entry_script='train.py',\n",
    "                source_directory=script_folder,                 \n",
    "                environment_definition=conda_env,\n",
    "                compute_target=compute_target)\n",
    "\n",
    "# For ease of understanding, and compatiblity with existing code, I would convert the train.py to instead retrive\n",
    "# the argument using argparse versus Run.get_context().input_datasets['...']\n",
    "est_step = EstimatorStep(name='train step',\n",
    "                         estimator=est,\n",
    "                         estimator_entry_script_arguments=[prepared_fashion_ds.read_delimited_files().as_input(name='prepared_fashion_ds')],\n",
    "                         compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py).\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py#submit-config--tags-none----kwargs-). When submit is called, a [PipelineRun](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step prepare step [93bb3761][5c601cf1-fb39-4341-b4c7-3204499458eb], (This step will run and generate new outputs)\n",
      "Created step train step [d2d5ae79][985859b0-9482-4523-aa6f-d8ba4a1d2ce2], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 6daf7713-76d8-4767-a275-bc27f71ca2f1\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/keras-mnist-fashion/runs/6daf7713-76d8-4767-a275-bc27f71ca2f1?wsid=/subscriptions/321cae6f-e4d3-40bf-824f-c07493a62af5/resourcegroups/aml-testing-scus/workspaces/playground-scus\n"
     ]
    }
   ],
   "source": [
    "# build pipeline & run experiment\n",
    "pipeline = Pipeline(workspace, steps=[prep_step, est_step])\n",
    "run = exp.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the PipelineRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 6daf7713-76d8-4767-a275-bc27f71ca2f1\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/keras-mnist-fashion/runs/6daf7713-76d8-4767-a275-bc27f71ca2f1?wsid=/subscriptions/321cae6f-e4d3-40bf-824f-c07493a62af5/resourcegroups/aml-testing-scus/workspaces/playground-scus\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 7421c6d6-8cef-4788-8fdb-e49abd914f0e\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/keras-mnist-fashion/runs/7421c6d6-8cef-4788-8fdb-e49abd914f0e?wsid=/subscriptions/321cae6f-e4d3-40bf-824f-c07493a62af5/resourcegroups/aml-testing-scus/workspaces/playground-scus\n",
      "StepRun( prepare step ) Status: NotStarted\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "StepRun( prepare step ) Status: Running\n",
      "2020/06/16 16:47:46 Downloading source code...\n",
      "2020/06/16 16:47:48 Finished downloading source code\n",
      "2020/06/16 16:47:49 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2020/06/16 16:47:49 Successfully set up Docker network: acb_default_network\n",
      "2020/06/16 16:47:49 Setting up Docker configuration...\n",
      "2020/06/16 16:47:50 Successfully set up Docker configuration\n",
      "2020/06/16 16:47:50 Logging in to registry: playgroundsc2dcf87a3.azurecr.io\n",
      "2020/06/16 16:47:51 Successfully logged into playgroundsc2dcf87a3.azurecr.io\n",
      "2020/06/16 16:47:51 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/06/16 16:47:51 Scanning for dependencies...\n",
      "2020/06/16 16:47:53 Successfully scanned dependencies\n",
      "2020/06/16 16:47:53 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  60.93kB\n",
      "\n",
      "Step 1/15 : FROM mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200423.v1@sha256:a8f6491296cbc183e95e9de29b59098e97d50ea87b4f1afa93e2afd43afeaf6d\n",
      "sha256:a8f6491296cbc183e95e9de29b59098e97d50ea87b4f1afa93e2afd43afeaf6d: Pulling from azureml/intelmpi2018.3-ubuntu16.04\n",
      "fe703b657a32: Pulling fs layer\n",
      "f9df1fafd224: Pulling fs layer\n",
      "a645a4b887f9: Pulling fs layer\n",
      "57db7fe0b522: Pulling fs layer\n",
      "20b5fabe4f63: Pulling fs layer\n",
      "22898513a7dc: Pulling fs layer\n",
      "b77f65fcd9d7: Pulling fs layer\n",
      "132ebd5cd5ca: Pulling fs layer\n",
      "01991399be72: Pulling fs layer\n",
      "60c58ca14ef7: Pulling fs layer\n",
      "ca339bb1ce1b: Pulling fs layer\n",
      "57db7fe0b522: Waiting\n",
      "132ebd5cd5ca: Waiting\n",
      "01991399be72: Waiting\n",
      "20b5fabe4f63: Waiting\n",
      "60c58ca14ef7: Waiting\n",
      "ca339bb1ce1b: Waiting\n",
      "22898513a7dc: Waiting\n",
      "b77f65fcd9d7: Waiting\n",
      "a645a4b887f9: Verifying Checksum\n",
      "a645a4b887f9: Download complete\n",
      "f9df1fafd224: Verifying Checksum\n",
      "f9df1fafd224: Download complete\n",
      "57db7fe0b522: Verifying Checksum\n",
      "57db7fe0b522: Download complete\n",
      "fe703b657a32: Verifying Checksum\n",
      "fe703b657a32: Download complete\n",
      "b77f65fcd9d7: Verifying Checksum\n",
      "b77f65fcd9d7: Download complete\n",
      "22898513a7dc: Verifying Checksum\n",
      "22898513a7dc: Download complete\n",
      "20b5fabe4f63: Verifying Checksum\n",
      "20b5fabe4f63: Download complete\n",
      "60c58ca14ef7: Verifying Checksum\n",
      "60c58ca14ef7: Download complete\n",
      "ca339bb1ce1b: Verifying Checksum\n",
      "ca339bb1ce1b: Download complete\n",
      "01991399be72: Verifying Checksum\n",
      "01991399be72: Download complete\n",
      "132ebd5cd5ca: Verifying Checksum\n",
      "132ebd5cd5ca: Download complete\n",
      "fe703b657a32: Pull complete\n",
      "f9df1fafd224: Pull complete\n",
      "a645a4b887f9: Pull complete\n",
      "57db7fe0b522: Pull complete\n",
      "20b5fabe4f63: Pull complete\n",
      "22898513a7dc: Pull complete\n",
      "b77f65fcd9d7: Pull complete\n",
      "132ebd5cd5ca: Pull complete\n",
      "01991399be72: Pull complete\n",
      "60c58ca14ef7: Pull complete\n",
      "ca339bb1ce1b: Pull complete\n",
      "Digest: sha256:a8f6491296cbc183e95e9de29b59098e97d50ea87b4f1afa93e2afd43afeaf6d\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200423.v1@sha256:a8f6491296cbc183e95e9de29b59098e97d50ea87b4f1afa93e2afd43afeaf6d\n",
      " ---> a0dab4f7c804\n",
      "Step 2/15 : USER root\n",
      " ---> Running in 913acfae3f47\n",
      "Removing intermediate container 913acfae3f47\n",
      " ---> 9775e3ac23b2\n",
      "Step 3/15 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in c80123cf76c9\n",
      "Removing intermediate container c80123cf76c9\n",
      " ---> 84b85642922b\n",
      "Step 4/15 : WORKDIR /\n",
      " ---> Running in b969d26639a3\n",
      "Removing intermediate container b969d26639a3\n",
      " ---> 33c67c17c250\n",
      "Step 5/15 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 7226b93421ff\n",
      "Step 6/15 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in af5dea37ad7a\n",
      "Removing intermediate container af5dea37ad7a\n",
      " ---> f443f6c2f238\n",
      "Step 7/15 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> 7bd15217f280\n",
      "Step 8/15 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in d5b01ea1e4ea\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... \n",
      "done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ncurses-6.0          | 907 KB    |            |   0% \n",
      "ncurses-6.0          | 907 KB    | #4         |  14% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "\n",
      "ca-certificates-2020 | 132 KB    |            |   0% \n",
      "ca-certificates-2020 | 132 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.0.2u       | 3.1 MB    |            |   0% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "\n",
      "libffi-3.2.1         | 43 KB     |            |   0% \n",
      "libffi-3.2.1         | 43 KB     | ########## | 100% \n",
      "\n",
      "tk-8.6.8             | 3.1 MB    |            |   0% \n",
      "tk-8.6.8             | 3.1 MB    | ########## | 100% \n",
      "\n",
      "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
      "\n",
      "pip-20.0.2           | 1.9 MB    |            |   0% \n",
      "pip-20.0.2           | 1.9 MB    | ########## | 100% \n",
      "\n",
      "readline-7.0         | 387 KB    |            |   0% \n",
      "readline-7.0         | 387 KB    | ########## | 100% \n",
      "\n",
      "xz-5.2.5             | 438 KB    |            |   0% \n",
      "xz-5.2.5             | 438 KB    | ########## | 100% \n",
      "\n",
      "libedit-3.1          | 171 KB    |            |   0% \n",
      "libedit-3.1          | 171 KB    | ########## | 100% \n",
      "\n",
      "certifi-2020.4.5.1   | 159 KB    |            |   0% \n",
      "certifi-2020.4.5.1   | 159 KB    | ########## | 100% \n",
      "\n",
      "python-3.6.2         | 27.0 MB   |            |   0% \n",
      "python-3.6.2         | 27.0 MB   | ##7        |  28% \n",
      "python-3.6.2         | 27.0 MB   | ######7    |  67% \n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "\n",
      "sqlite-3.23.1        | 1.5 MB    |            |   0% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "\n",
      "zlib-1.2.11          | 120 KB    |            |   0% \n",
      "zlib-1.2.11          | 120 KB    | ########## | 100% \n",
      "\n",
      "setuptools-47.1.1    | 653 KB    |            |   0% \n",
      "setuptools-47.1.1    | 653 KB    | ########## | 100% \n",
      "\n",
      "wheel-0.34.2         | 49 KB     |            |   0% \n",
      "wheel-0.34.2         | 49 KB     | ########## | 100% \n",
      "\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Ran pip subprocess with arguments:\n",
      "['/azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.w5r9lilw.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Looking in indexes: https://azuremlsdktestpypi.azureedge.net/Create-Dev-Index/15335858/, https://pypi.python.org/simple\n",
      "Collecting azureml-sdk~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_sdk-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=nomedaiGVm0jf5kO3aBLsnc%2BkwWxx6XSbHpTwrvOpqo%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (4.5 kB)\n",
      "Collecting azureml-pipeline~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_pipeline-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=bSFg6CxEgB4FFo1nF%2FsXRLyS28j%2BRU4xGOZqKtt7Ifs%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (3.8 kB)\n",
      "Collecting azureml-dataprep[fuse]<1.8.0a,>=1.7.0a\n",
      "  Downloading https://dataprepdownloads.azureedge.net/install/wheels/daily-001996AEA44B4B4CA18D726613/0.1.2006.12011/win/ship/azureml_dataprep-1.7.1a2020061201-py3-none-any.whl (27.7 MB)\n",
      "Collecting azureml-core~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_core-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=97xOtg2oFrgq%2BErsjOT2ciIDTMDZDBC2FrCfVLWS0tw%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (1.5 MB)\n",
      "Collecting azureml-train-automl-client~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_train_automl_client-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=Ic5TkO3ykISOZZFL7jX0g9VCY3E3x0tpHn676t4V5I0%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (89 kB)\n",
      "Collecting azureml-train~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_train-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=n1Ek8qHb3Tt%2BfnjewQycqQR5W%2BBZv6vrFXdZ6depJW8%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (3.4 kB)\n",
      "Collecting azureml-pipeline-steps~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_pipeline_steps-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=5iWWFeYaaUblg%2BErIf2FbDIv9wR3faGLe7w1kDJIT2k%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (58 kB)\n",
      "Collecting azureml-pipeline-core~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_pipeline_core-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=y%2BmyEd9AULhR0pHGNkQGBqTSky%2FYyt4TbSveBV%2FznAY%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (294 kB)\n",
      "Collecting dotnetcore2>=2.1.14\n",
      "  Downloading dotnetcore2-2.1.14-py3-none-manylinux1_x86_64.whl (29.3 MB)\n",
      "Collecting azure-identity<1.3.0,>=1.2.0\n",
      "  Downloading azure_identity-1.2.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting cloudpickle>=1.1.0\n",
      "  Downloading cloudpickle-1.4.1-py3-none-any.whl (26 kB)\n",
      "Collecting azureml-dataprep-native<15.0.0,>=14.2.0\n",
      "  Downloading azureml_dataprep_native-14.2.1-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting fusepy>=3.0.1; extra == \"fuse\"\n",
      "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0\n",
      "  Downloading azure_mgmt_containerregistry-3.0.0rc13-py2.py3-none-any.whl (538 kB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\n",
      "  Downloading cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Collecting PyJWT\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting adal>=1.2.0\n",
      "  Downloading adal-1.2.4-py2.py3-none-any.whl (55 kB)\n",
      "Collecting pathspec\n",
      "  Downloading pathspec-0.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting jsonpickle\n",
      "  Downloading jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting azure-mgmt-keyvault>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-2.2.0-py2.py3-none-any.whl (89 kB)\n",
      "Collecting azure-graphrbac>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "Collecting azure-mgmt-authorization>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-0.60.0-py2.py3-none-any.whl (82 kB)\n",
      "Collecting ruamel.yaml>0.16.7\n",
      "  Downloading ruamel.yaml-0.16.10-py2.py3-none-any.whl (111 kB)\n",
      "Collecting requests>=2.19.1\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting azure-mgmt-storage>=1.5.0\n",
      "  Downloading azure_mgmt_storage-10.0.0-py2.py3-none-any.whl (532 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting azure-common>=1.1.12\n",
      "  Downloading azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\n",
      "Collecting msrest>=0.5.1\n",
      "  Downloading msrest-0.6.16-py2.py3-none-any.whl (84 kB)\n",
      "Collecting SecretStorage\n",
      "  Downloading SecretStorage-3.1.2-py3-none-any.whl (14 kB)\n",
      "Collecting urllib3>=1.23\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting ndg-httpsclient\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting pyopenssl\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting jmespath\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting docker\n",
      "  Downloading docker-4.2.1-py2.py3-none-any.whl (143 kB)\n",
      "Collecting msrestazure>=0.4.33\n",
      "  Downloading msrestazure-0.6.3-py2.py3-none-any.whl (40 kB)\n",
      "Collecting azure-mgmt-network~=10.0\n",
      "  Downloading azure_mgmt_network-10.2.0-py2.py3-none-any.whl (8.6 MB)\n",
      "Collecting azure-mgmt-resource>=1.2.1\n",
      "  Downloading azure_mgmt_resource-10.0.0-py2.py3-none-any.whl (809 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting azureml-automl-core~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_automl_core-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=7FqMvSAnFnsTly9%2FHMVZ%2FGEYPnQMQdHwOUqZITYYnoY%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (131 kB)\n",
      "Collecting azureml-telemetry~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_telemetry-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=eIzWxBsgOg3EpVYOYtFCC51OIpqOXnnG6BuR7i%2BuAhM%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (30 kB)\n",
      "Collecting azureml-train-core~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_train_core-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=F%2FYImZBn1h85iTst1nsgGMfWVNwsGnrxhLqTiZ%2Fl3bQ%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (8.6 MB)\n",
      "Collecting distro>=1.2.0\n",
      "  Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting msal<2.0.0,>=1.0.0\n",
      "  Downloading msal-1.3.0-py2.py3-none-any.whl (48 kB)\n",
      "Collecting six>=1.6\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting msal-extensions~=0.1.3\n",
      "  Downloading msal_extensions-0.1.3-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting azure-core<2.0.0,>=1.0.0\n",
      "  Downloading azure_core-1.6.0-py2.py3-none-any.whl (120 kB)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Downloading cffi-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (399 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-1.6.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
      "  Downloading ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb/lib/python3.6/site-packages (from requests>=2.19.1->azureml-core~=0.1.0.15772943->azureml-sdk~=0.1.0.15772943->-r /azureml-environment-setup/condaenv.w5r9lilw.requirements.txt (line 3)) (2020.4.5.1)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Collecting jeepney>=0.4.2\n",
      "  Downloading jeepney-0.4.3-py3-none-any.whl (21 kB)\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting applicationinsights\n",
      "  Downloading applicationinsights-0.11.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"\n",
      "  Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
      "Collecting azureml-train-restclients-hyperdrive~=0.1.0.15772943\n",
      "  Downloading https://azuremlsdktestpypi.blob.core.windows.net/repo/Create-Dev-Index/15335858/azureml_train_restclients_hyperdrive-0.1.0.15772943-py3-none-any.whl?sv=2018-03-28&sr=b&sig=40EL%2BXevYQCR17lqb4XZksZrJPn4Qv1bqgabTVA5q48%3D&st=2020-06-16T01%3A25%3A36Z&se=2021-06-16T01%3A25%3A36Z&sp=rl (19 kB)\n",
      "Collecting portalocker~=1.0\n",
      "  Downloading portalocker-1.7.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyflakes<2.2.0,>=2.1.0\n",
      "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting entrypoints<0.4.0,>=0.3.0\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pycodestyle<2.6.0,>=2.5.0\n",
      "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
      "Building wheels for collected packages: fusepy\n",
      "  Building wheel for fusepy (setup.py): started\n",
      "  Building wheel for fusepy (setup.py): finished with status 'done'\n",
      "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10503 sha256=f00ff34e613830f4f8aa69f1b7210a3357620cc4b88007c352e33edc46ad2f4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/5c/83/1dd7e8a232d12227e5410120f4374b33adeb4037473105b079\n",
      "Successfully built fusepy\n",
      "Installing collected packages: distro, dotnetcore2, urllib3, chardet, idna, requests, PyJWT, msal, six, pycparser, cffi, cryptography, portalocker, msal-extensions, azure-core, azure-identity, cloudpickle, azureml-dataprep-native, fusepy, azureml-dataprep, pytz, applicationinsights, azure-common, oauthlib, requests-oauthlib, isodate, msrest, python-dateutil, adal, msrestazure, azure-mgmt-containerregistry, pathspec, zipp, importlib-metadata, jsonpickle, contextlib2, azure-mgmt-keyvault, azure-graphrbac, azure-mgmt-authorization, ruamel.yaml.clib, ruamel.yaml, azure-mgmt-storage, jeepney, SecretStorage, backports.weakref, backports.tempfile, pyopenssl, pyasn1, ndg-httpsclient, jmespath, websocket-client, docker, azure-mgmt-network, azure-mgmt-resource, azureml-core, azureml-telemetry, azureml-automl-core, azureml-train-automl-client, azureml-pipeline-core, pyflakes, mccabe, entrypoints, pycodestyle, flake8, azureml-train-restclients-hyperdrive, azureml-train-core, azureml-pipeline-steps, azureml-pipeline, azureml-train, azureml-sdk\n",
      "Successfully installed PyJWT-1.7.1 SecretStorage-3.1.2 adal-1.2.4 applicationinsights-0.11.9 azure-common-1.1.25 azure-core-1.6.0 azure-graphrbac-0.61.1 azure-identity-1.2.0 azure-mgmt-authorization-0.60.0 azure-mgmt-containerregistry-3.0.0rc13 azure-mgmt-keyvault-2.2.0 azure-mgmt-network-10.2.0 azure-mgmt-resource-10.0.0 azure-mgmt-storage-10.0.0 azureml-automl-core-0.1.0.15772943 azureml-core-0.1.0.15772943 azureml-dataprep-1.7.1a2020061201 azureml-dataprep-native-14.2.1 azureml-pipeline-0.1.0.15772943 azureml-pipeline-core-0.1.0.15772943 azureml-pipeline-steps-0.1.0.15772943 azureml-sdk-0.1.0.15772943 azureml-telemetry-0.1.0.15772943 azureml-train-0.1.0.15772943 azureml-train-automl-client-0.1.0.15772943 azureml-train-core-0.1.0.15772943 azureml-train-restclients-hyperdrive-0.1.0.15772943 backports.tempfile-1.0 backports.weakref-1.0.post1 cffi-1.14.0 chardet-3.0.4 cloudpickle-1.4.1 contextlib2-0.6.0.post1 cryptography-2.9.2 distro-1.5.0 docker-4.2.1 dotnetcore2-2.1.14 entrypoints-0.3 flake8-3.7.9 fusepy-3.0.1 idna-2.9 importlib-metadata-1.6.1 isodate-0.6.0 jeepney-0.4.3 jmespath-0.10.0 jsonpickle-1.4.1 mccabe-0.6.1 msal-1.3.0 msal-extensions-0.1.3 msrest-0.6.16 msrestazure-0.6.3 ndg-httpsclient-0.5.1 oauthlib-3.1.0 pathspec-0.8.0 portalocker-1.7.0 pyasn1-0.4.8 pycodestyle-2.5.0 pycparser-2.20 pyflakes-2.1.1 pyopenssl-19.1.0 python-dateutil-2.8.1 pytz-2020.1 requests-2.23.0 requests-oauthlib-1.3.0 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0 six-1.15.0 urllib3-1.25.9 websocket-client-0.57.0 zipp-3.1.0\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "WARNING: /root/.conda/pkgs does not exist\n",
      "\n",
      "Removing intermediate container d5b01ea1e4ea\n",
      " ---> c0bd6cb5a182\n",
      "Step 9/15 : ENV PATH /azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb/bin:$PATH\n",
      " ---> Running in d4905789dba8\n",
      "Removing intermediate container d4905789dba8\n",
      " ---> ceca911f3931\n",
      "Step 10/15 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb\n",
      " ---> Running in d8ccd4bb63c3\n",
      "Removing intermediate container d8ccd4bb63c3\n",
      " ---> 0f14ede4cf1c\n",
      "Step 11/15 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_5ba6c4fcc9639511f713974b4de9c2cb/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in e68029c0374d\n",
      "Removing intermediate container e68029c0374d\n",
      " ---> 8e29e51f759f\n",
      "Step 12/15 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> 2217d1517595\n",
      "Step 13/15 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
      " ---> Running in ea3ba54ade85\n",
      "Removing intermediate container ea3ba54ade85\n",
      " ---> c8cc7b97a945\n",
      "Step 14/15 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in 49272837528b\n",
      "Removing intermediate container 49272837528b\n",
      " ---> 410952e066b7\n",
      "Step 15/15 : CMD [\"bash\"]\n",
      " ---> Running in 52efb6adc154\n",
      "Removing intermediate container 52efb6adc154\n",
      " ---> 8471ff917616\n",
      "Successfully built 8471ff917616\n",
      "Successfully tagged playgroundsc2dcf87a3.azurecr.io/azureml/azureml_9963f9ae719a3d6a6aa297575456a261:latest\n",
      "2020/06/16 16:50:13 Successfully executed container: acb_step_0\n",
      "2020/06/16 16:50:13 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/06/16 16:50:13 Pushing image: playgroundsc2dcf87a3.azurecr.io/azureml/azureml_9963f9ae719a3d6a6aa297575456a261:latest, attempt 1\n",
      "The push refers to repository [playgroundsc2dcf87a3.azurecr.io/azureml/azureml_9963f9ae719a3d6a6aa297575456a261]\n",
      "7c2e92240190: Preparing\n",
      "2db2486ead68: Preparing\n",
      "38e079381592: Preparing\n",
      "540a468585c8: Preparing\n",
      "1b703f78f401: Preparing\n",
      "680a30770c31: Preparing\n",
      "f5aa53607bc1: Preparing\n",
      "07eae5ad8c0b: Preparing\n",
      "facf43cddd83: Preparing\n",
      "69a9bdc813b0: Preparing\n",
      "7e2b9752143f: Preparing\n",
      "63a67842a4c7: Preparing\n",
      "ae3a847dbd6b: Preparing\n",
      "4ae3adcb66cb: Preparing\n",
      "aa6685385151: Preparing\n",
      "0040d8f00d7e: Preparing\n",
      "9e6f810a2aab: Preparing\n",
      "680a30770c31: Waiting\n",
      "f5aa53607bc1: Waiting\n",
      "07eae5ad8c0b: Waiting\n",
      "facf43cddd83: Waiting\n",
      "69a9bdc813b0: Waiting\n",
      "7e2b9752143f: Waiting\n",
      "63a67842a4c7: Waiting\n",
      "ae3a847dbd6b: Waiting\n",
      "4ae3adcb66cb: Waiting\n",
      "aa6685385151: Waiting\n",
      "0040d8f00d7e: Waiting\n",
      "9e6f810a2aab: Waiting\n",
      "7c2e92240190: Pushed\n",
      "540a468585c8: Pushed\n",
      "1b703f78f401: Pushed\n",
      "38e079381592: Pushed\n",
      "\n",
      "07eae5ad8c0b: Pushed\n",
      "680a30770c31: Pushed\n",
      "f5aa53607bc1: Pushed\n",
      "2db2486ead68: Pushed\n",
      "63a67842a4c7: Pushed\n",
      "4ae3adcb66cb: Pushed\n",
      "aa6685385151: Pushed\n",
      "0040d8f00d7e: Pushed\n",
      "7e2b9752143f: Pushed\n",
      "69a9bdc813b0: Pushed\n",
      "ae3a847dbd6b: Pushed\n",
      "facf43cddd83: Pushed\n",
      "9e6f810a2aab: Pushed\n",
      "latest: digest: sha256:39e020c99480b94934bb436dbc8a97757865e268a9053d25499ca6061f0c75bb size: 3883\n",
      "2020/06/16 16:51:23 Successfully pushed image: playgroundsc2dcf87a3.azurecr.io/azureml/azureml_9963f9ae719a3d6a6aa297575456a261:latest\n",
      "2020/06/16 16:51:23 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 142.020467)\n",
      "2020/06/16 16:51:23 Populating digests for step ID: acb_step_0...\n",
      "2020/06/16 16:51:25 Successfully populated digests for step ID: acb_step_0\n",
      "2020/06/16 16:51:25 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 70.832097)\n",
      "2020/06/16 16:51:25 The following dependencies were found:\n",
      "2020/06/16 16:51:25 \n",
      "- image:\n",
      "    registry: playgroundsc2dcf87a3.azurecr.io\n",
      "    repository: azureml/azureml_9963f9ae719a3d6a6aa297575456a261\n",
      "    tag: latest\n",
      "    digest: sha256:39e020c99480b94934bb436dbc8a97757865e268a9053d25499ca6061f0c75bb\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/intelmpi2018.3-ubuntu16.04\n",
      "    tag: 20200423.v1\n",
      "    digest: sha256:a8f6491296cbc183e95e9de29b59098e97d50ea87b4f1afa93e2afd43afeaf6d\n",
      "  git: {}\n",
      "\n",
      "Run ID: cd7 was successful after 3m40s\n"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.find_step_run('train step')[0].get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the input dataset and the output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Machine Learning dataset makes it easy to trace how your data is used in ML. [Learn More](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-version-track-datasets#track-datasets-in-experiments)<br>\n",
    "For each Machine Learning experiment, you can easily trace the datasets used as the input through `Run` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input datasets\n",
    "prep_step = run.find_step_run('prepare step')[0]\n",
    "# Why not a method to directly get input datasets? in addition to the get_details()... something like step.get_input_datasets(), or get_output_datasets()\n",
    "inputs = prep_step.get_details()['inputDatasets']\n",
    "input_dataset = inputs[0]['dataset']\n",
    "\n",
    "# list the files referenced by input_dataset\n",
    "input_dataset.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the input Fashion MNIST dataset with the workspace so that you can reuse it in other experiments or share it with your colleagues who have access to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_ds = input_dataset.register(workspace = workspace,\n",
    "                                    name = 'fashion_ds',\n",
    "                                    description = 'image and label files from fashion mnist',\n",
    "                                    create_new_version = True)\n",
    "fashion_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the output model with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.find_step_run('train step')[0].register_model(model_name = 'keras-model', model_path = 'outputs/model/', \n",
    "                                                  datasets =[('train test data',fashion_ds)])"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Fashion MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Datasets with ML Pipeline",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "star_tag": [
   "featured"
  ],
  "tags": [
   "Dataset",
   "Pipeline",
   "Estimator",
   "ScriptRun"
  ],
  "task": "Train"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
